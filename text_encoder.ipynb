{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.doubanio.com/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already up-to-date: transformers in /root/.local/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /root/.local/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /root/.local/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /root/.local/lib/python3.6/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /root/.local/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub==0.0.8 in /root/.local/lib/python3.6/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /root/.local/lib/python3.6/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: click in /root/.local/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /root/.local/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.doubanio.com/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already up-to-date: pandarallel in /root/.local/lib/python3.6/site-packages (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: dill in /root/.local/lib/python3.6/site-packages (from pandarallel) (0.3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.190861Z",
     "start_time": "2021-05-16T12:21:26.464606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pandarallel import pandarallel\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pandarallel.initialize()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.197000Z",
     "start_time": "2021-05-16T12:21:27.193297Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('data/tmp', exist_ok=True)\n",
    "os.makedirs('data/embedding', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.324528Z",
     "start_time": "2021-05-16T12:21:27.199750Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.434638Z",
     "start_time": "2021-05-16T12:21:27.326665Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self,\n",
    "                 sentences,\n",
    "                 embeddings,\n",
    "                 key_name,\n",
    "                 prefix,\n",
    "                 keys_sentences_map=None):\n",
    "        self.sentences = sentences\n",
    "        self.embeddings = embeddings\n",
    "        self.key_name = key_name\n",
    "        self.prefix = prefix\n",
    "\n",
    "        if keys_sentences_map is not None:\n",
    "            self.keys_sentences_map = keys_sentences_map\n",
    "        else:\n",
    "            self.keys_sentences_map = dict(zip(sentences, sentences))\n",
    "\n",
    "        sentences_embeddings_map = dict(zip(sentences, embeddings))\n",
    "        self.keys_embeddings_map = {}\n",
    "        for key, sentence in self.keys_sentences_map.items():\n",
    "            self.keys_embeddings_map[key] = sentences_embeddings_map[sentence]\n",
    "\n",
    "    def get_embeddings(self, normalize=False):\n",
    "        if normalize and self.keys_normalize_embeddings_map is not None:\n",
    "            keys_embeddings_map = self.keys_normalize_embeddings_map\n",
    "        else:\n",
    "            keys_embeddings_map = self.keys_embeddings_map\n",
    "\n",
    "        emb_size = len(list(keys_embeddings_map.values())[0])\n",
    "\n",
    "        data_list = []\n",
    "        for key, embedding in keys_embeddings_map.items():\n",
    "            data_list.append([key] + list(embedding))\n",
    "\n",
    "        df_emb = pd.DataFrame(data_list)\n",
    "        df_emb.columns = [self.key_name] + [\n",
    "            '{}_emb_{}'.format(self.prefix, i) for i in range(emb_size)\n",
    "        ]\n",
    "\n",
    "        return df_emb\n",
    "\n",
    "    def get_embedding(self, key, normalize=False):\n",
    "        try:\n",
    "            if normalize and self.keys_normalize_embeddings_map is not None:\n",
    "                return self.keys_normalize_embeddings_map[key]\n",
    "            else:\n",
    "                return self.keys_embeddings_map[key]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def transform_and_normalize(self, kernel, bias, n_components=None):\n",
    "        \"\"\"应用变换，然后标准化\n",
    "        \"\"\"\n",
    "        if n_components is not None:\n",
    "            kernel = kernel[:, :n_components]\n",
    "\n",
    "        if not (kernel is None or bias is None):\n",
    "            vecs = (self.embeddings + bias).dot(kernel)\n",
    "        else:\n",
    "            vecs = vecs\n",
    "\n",
    "        vecs = vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n",
    "\n",
    "        sentences_embeddings_map = dict(zip(self.sentences, vecs))\n",
    "        self.keys_normalize_embeddings_map = {}\n",
    "        for key, sentence in self.keys_sentences_map.items():\n",
    "            self.keys_normalize_embeddings_map[key] = sentences_embeddings_map[\n",
    "                sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.569934Z",
     "start_time": "2021-05-16T12:21:27.437197Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def build_model(path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModel.from_pretrained(path)\n",
    "    model = model.to(DEVICE)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.677596Z",
     "start_time": "2021-05-16T12:21:27.572566Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sent_to_vec(sent, tokenizer, model, pooling, max_length):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sent,\n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=max_length)\n",
    "        inputs['input_ids'] = inputs['input_ids'].to(DEVICE)\n",
    "        inputs['token_type_ids'] = inputs['token_type_ids'].to(DEVICE)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(DEVICE)\n",
    "\n",
    "        hidden_states = model(**inputs,\n",
    "                              return_dict=True,\n",
    "                              output_hidden_states=True).hidden_states\n",
    "\n",
    "        if pooling == 'first_last_avg':\n",
    "            output_hidden_state = (hidden_states[-1] +\n",
    "                                   hidden_states[1]).mean(dim=1)\n",
    "        elif pooling == 'last_avg':\n",
    "            output_hidden_state = (hidden_states[-1]).mean(dim=1)\n",
    "        elif pooling == 'last2avg':\n",
    "            output_hidden_state = (hidden_states[-1] +\n",
    "                                   hidden_states[-2]).mean(dim=1)\n",
    "        elif pooling == 'cls':\n",
    "            output_hidden_state = (hidden_states[-1])[:, 0, :]\n",
    "        else:\n",
    "            raise Exception(\"unknown pooling {}\".format(POOLING))\n",
    "\n",
    "        vec = output_hidden_state.cpu().numpy()[0]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.788011Z",
     "start_time": "2021-05-16T12:21:27.680147Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sents_to_vecs(sents, tokenizer, model, pooling, max_length, verbose=True):\n",
    "    vecs = []\n",
    "    if verbose:\n",
    "        sents = tqdm(sents)\n",
    "    for sent in sents:\n",
    "        vec = sent_to_vec(sent, tokenizer, model, pooling, max_length)\n",
    "        vecs.append(vec)\n",
    "    assert len(sents) == len(vecs)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:27.915916Z",
     "start_time": "2021-05-16T12:21:27.791095Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_kernel_bias(vecs):\n",
    "    \"\"\"计算kernel和bias\n",
    "    最后的变换：y = (x + bias).dot(kernel)\n",
    "    \"\"\"\n",
    "    vecs = np.concatenate(vecs, axis=0)\n",
    "    mu = vecs.mean(axis=0, keepdims=True)\n",
    "    cov = np.cov(vecs.T)\n",
    "    u, s, vh = np.linalg.svd(cov)\n",
    "    W = np.dot(u, np.diag(s**0.5))\n",
    "    W = np.linalg.inv(W.T)\n",
    "    return W, -mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.304643Z",
     "start_time": "2021-05-16T12:21:27.920504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data/pretrain_models/ernie were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "path = 'data/pretrain_models/ernie'\n",
    "tokenizer, model = build_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.310720Z",
     "start_time": "2021-05-16T12:21:31.307640Z"
    }
   },
   "outputs": [],
   "source": [
    "vecs_list = []\n",
    "pooling = 'cls'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.424797Z",
     "start_time": "2021-05-16T12:21:31.312609Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 招聘岗位信息的招聘职位\n",
    "def get_job_title_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/job_title_vecs.npy')\n",
    "        with open('data/tmp/job_title_encoder.txt', 'rb') as f:\n",
    "            job_title_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_recruit = pd.read_csv('data/trainset/recruit.csv')\n",
    "        sentences = df_recruit['JOB_TITLE'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        job_title_encoder = Encoder(sentences, vecs, 'JOB_TITLE',\n",
    "                                    'JOB_TITLE_ernie')\n",
    "\n",
    "        np.save('data/tmp/job_title_vecs.npy', vecs)\n",
    "        with open('data/tmp/job_title_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(job_title_encoder, f)\n",
    "\n",
    "    return vecs, job_title_encoder\n",
    "\n",
    "\n",
    "vecs, job_title_encoder = get_job_title_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.509510Z",
     "start_time": "2021-05-16T12:21:31.427430Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "def major_clean(x):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "\n",
    "    x = x.replace('【', '').replace('】', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.613544Z",
     "start_time": "2021-05-16T12:21:31.512154Z"
    }
   },
   "outputs": [],
   "source": [
    "# 招聘岗位信息的对应聘者的专业要求\n",
    "def get_recruit_major_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/recruit_major_vecs.npy')\n",
    "        with open('data/tmp/recruit_major_encoder.txt', 'rb') as f:\n",
    "            recruit_major_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_recruit = pd.read_csv('data/trainset/recruit.csv')\n",
    "        df_recruit['MAJOR'].fillna('', inplace=True)\n",
    "        df_recruit['MAJOR'] = df_recruit['MAJOR'].apply(major_clean)\n",
    "        sentences = df_recruit['MAJOR'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        recruit_major_encoder = Encoder(sentences, vecs, 'MAJOR',\n",
    "                                        'recruit_MAJOR_ernie')\n",
    "\n",
    "        np.save('data/tmp/recruit_major_vecs.npy', vecs)\n",
    "        with open('data/tmp/recruit_major_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(recruit_major_encoder, f)\n",
    "\n",
    "    return vecs, recruit_major_encoder\n",
    "\n",
    "\n",
    "vecs, recruit_major_encoder = get_recruit_major_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "def detail_clean(x):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "\n",
    "    x = x.replace('****', '').replace('~', '').replace('-','').replace('')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf 招聘岗位信息的工作职责\n",
    "def get_recruit_detail_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/recruit_detail_vecs.npy')\n",
    "        with open('data/tmp/recruit_detail_encoder.txt', 'rb') as f:\n",
    "            recruit_major_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_recruit = pd.read_csv('data/trainset/recruit.csv')\n",
    "        df_recruit['DETAIL'].fillna('', inplace=True)\n",
    "        df_recruit['DETAIL'] = df_recruit['DETAIL'].apply(detail_clean)\n",
    "        sentences = df_recruit['DETAIL'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        recruit_major_encoder = Encoder(sentences, vecs, 'DETAIL',\n",
    "                                        'recruit_DETAIL_ernie')\n",
    "\n",
    "        np.save('data/tmp/recruit_major_vecs.npy', vecs)\n",
    "        with open('data/tmp/recruit_major_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(recruit_major_encoder, f)\n",
    "\n",
    "    return vecs, recruit_major_encoder\n",
    "\n",
    "\n",
    "vecs, recruit_major_encoder = get_recruit_major_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:31.737153Z",
     "start_time": "2021-05-16T12:21:31.616305Z"
    }
   },
   "outputs": [],
   "source": [
    "# 求职者基本信息的应聘者专业\n",
    "def get_person_major_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/person_major_vecs.npy')\n",
    "        with open('data/tmp/person_major_encoder.txt', 'rb') as f:\n",
    "            person_major_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_person = pd.read_csv('data/trainset/person.csv')\n",
    "        df_person['MAJOR'].fillna('', inplace=True)\n",
    "        df_person['MAJOR'] = df_person['MAJOR'].apply(major_clean)\n",
    "        sentences = df_person['MAJOR'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        person_major_encoder = Encoder(sentences, vecs, 'MAJOR',\n",
    "                                       'person_MAJOR_ernie')\n",
    "\n",
    "        np.save('data/tmp/person_major_vecs.npy', vecs)\n",
    "        with open('data/tmp/person_major_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(person_major_encoder, f)\n",
    "\n",
    "    return vecs, person_major_encoder\n",
    "\n",
    "\n",
    "vecs, person_major_encoder = get_person_major_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "def lastposition_clean(x):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "\n",
    "    x = x.replace('*', '').replace('/', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wf求职者基本信息的应聘者上一份工作岗位\n",
    "def get_person_lastposition_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/person_lastposition_vecs.npy')\n",
    "        with open('data/tmp/person_lastposition_encoder.txt', 'rb') as f:\n",
    "            person_lastposition_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_person = pd.read_csv('data/trainset/person.csv')\n",
    "        df_person['LAST_POSITION'].fillna('', inplace=True)\n",
    "        df_person['LAST_POSITION'] = df_person['LAST_POSITION'].apply(lastposition_clean)\n",
    "        sentences = df_person['LAST_POSITION'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        person_lastposition_encoder = Encoder(sentences, vecs, 'LAST_POSITION',\n",
    "                                       'person_LAST_POSITION_ernie')\n",
    "\n",
    "        np.save('data/tmp/person_lastposition_vecs.npy', vecs)\n",
    "        with open('data/tmp/person_lastposition_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(person_lastposition_encoder, f)\n",
    "\n",
    "    return vecs, person_lastposition_encoder\n",
    "\n",
    "\n",
    "vecs, person_lasposition_encoder = get_person_lastposition_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "def cvposition_clean(x):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "\n",
    "    x = x.replace('*', '').replace('/', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wf求职者简历信息的投递职位\n",
    "def get_person_cvposition_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/person_cvposition_vecs.npy')\n",
    "        with open('data/tmp/person_cvposition_encoder.txt', 'rb') as f:\n",
    "            person_cvposition_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_person = pd.read_csv('data/trainset/person_cv.csv')\n",
    "        df_person['POSITION'].fillna('', inplace=True)\n",
    "        df_person['POSITION'] = df_person['POSITION'].apply(cvposition_clean)\n",
    "        tmp = df_person.groupby('PERSON_ID')['POSITION'].apply(lambda x:x.str.cat(sep='')).reset_index()\n",
    "        sentences = tmp.values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        person_cvposition_encoder = Encoder(sentences, vecs, 'POSITION',\n",
    "                                       'person_POSITION_ernie')\n",
    "\n",
    "        np.save('data/tmp/person_cvposition_vecs.npy', vecs)\n",
    "        with open('data/tmp/person_cvposition_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(person_cvposition_encoder, f)\n",
    "\n",
    "    return vecs, person_cvposition_encoder\n",
    "\n",
    "\n",
    "vecs, person_cvposition_encoder = get_person_cvposition_encoder()\n",
    "vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#求职者投递简历的自我介绍\n",
    "# wf\n",
    "def get_person_selfcomment_encoder():\n",
    "    try:\n",
    "        vecs = np.load('data/tmp/person_selfcomment_vecs.npy')\n",
    "        with open('data/tmp/person_selfcomment_encoder.txt', 'rb') as f:\n",
    "            person_selfcomment_encoder = pickle.load(f)\n",
    "\n",
    "    except Exception:\n",
    "        df_person = pd.read_csv('data/trainset/person_cv.csv')\n",
    "        \n",
    "        #tmp = df_person.groupby(['PERSON_ID'])['PRO_CERT_DSP'].apply(lambda x:x.str.cat(sep=' ')).reset_index()\n",
    "        #tmp.columns = ['PERSON_ID','PRO_CERT_DSP']\n",
    "        #tmp['PRO_CERT_DSP'].fillna('', inplace=True)\n",
    "        #sentences = tmp['PRO_CERT_DSP'].values.tolist()\n",
    "        df_person['SELF_COMMENT'].fillna('', inplace=True)\n",
    "        sentences = df_person['SELF_COMMENT'].values.tolist()\n",
    "        sentences = list(set(sentences))\n",
    "\n",
    "        vecs = sents_to_vecs(sentences, tokenizer, model, pooling, max_length)\n",
    "        person_selfcomment_encoder = Encoder(sentences, vecs, 'SELF_COMMENT',\n",
    "                                       'person_SELF_COMMENT_ernie')\n",
    "\n",
    "        np.save('data/tmp/person_selfcomment_vecs.npy', vecs)\n",
    "        with open('data/tmp/person_selfcomment_encoder.txt', 'wb') as f:\n",
    "            pickle.dump(person_selfcomment_encoder, f)\n",
    "\n",
    "    return vecs, person_selfcomment_encoder\n",
    "\n",
    "\n",
    "vecs, person_selfcomment_encoder = get_person_selfcomment_encoder()\n",
    "#vecs_list.append(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.1258273e-01, -1.4553578e-01, -5.2823836e-01, ...,\n",
       "         -5.5799925e-01, -1.1621797e+00,  2.7167436e-01],\n",
       "        [ 1.4431961e+00,  4.8142123e-01, -8.5353512e-01, ...,\n",
       "         -1.3246323e-01, -5.6695729e-01,  3.9856920e-01],\n",
       "        [ 1.2136361e-01,  4.9559134e-01, -8.1867927e-01, ...,\n",
       "         -3.1611624e-01, -9.8271406e-01, -4.7156900e-01],\n",
       "        ...,\n",
       "        [ 7.3841190e-01,  2.6713213e-01, -6.7786264e-01, ...,\n",
       "          1.6303787e-01, -1.2209930e+00,  1.4365354e-01],\n",
       "        [ 7.5565451e-01,  1.1729981e-02,  8.7025249e-01, ...,\n",
       "         -3.5121053e-01, -5.8748102e-01, -7.0887886e-02],\n",
       "        [ 8.9640278e-01,  8.4413867e-04, -4.0254822e-01, ...,\n",
       "          2.4465663e-02, -9.9738163e-01,  8.2489550e-01]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.44478586, -0.28655863, -0.10807729, ..., -0.90000695,\n",
       "         -0.52228886,  0.54808503],\n",
       "        [ 1.0355208 ,  0.40055448,  0.22379814, ..., -0.2156638 ,\n",
       "         -0.91299975, -0.19149743],\n",
       "        ...,\n",
       "        [ 0.83456594, -0.18841773,  0.5793377 , ..., -0.17040886,\n",
       "         -0.6033796 ,  0.70765954],\n",
       "        [ 1.6373688 , -0.07057258, -0.3403034 , ..., -1.034435  ,\n",
       "         -0.8637714 ,  0.71643645],\n",
       "        [ 1.0697303 , -0.6546923 , -0.04688627, ..., -0.7477723 ,\n",
       "         -0.3965463 ,  0.30490908]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.44478586, -0.28655863, -0.10807729, ..., -0.90000695,\n",
       "         -0.52228886,  0.54808503],\n",
       "        [ 1.0355208 ,  0.40055448,  0.22379814, ..., -0.2156638 ,\n",
       "         -0.91299975, -0.19149743],\n",
       "        ...,\n",
       "        [ 0.83456594, -0.18841773,  0.5793377 , ..., -0.17040886,\n",
       "         -0.6033796 ,  0.70765954],\n",
       "        [ 1.6373688 , -0.07057258, -0.3403034 , ..., -1.034435  ,\n",
       "         -0.8637714 ,  0.71643645],\n",
       "        [ 1.0697303 , -0.6546923 , -0.04688627, ..., -0.7477723 ,\n",
       "         -0.3965463 ,  0.30490908]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.23024471, -1.198324  , -0.23742962, ..., -0.77526647,\n",
       "         -0.879528  ,  0.23312499],\n",
       "        [ 0.2944864 ,  0.09308026,  0.29740515, ..., -0.43847075,\n",
       "         -0.8585222 , -0.5245079 ],\n",
       "        ...,\n",
       "        [ 0.8356476 , -0.42286742,  0.15158059, ..., -0.30628353,\n",
       "         -0.62029594,  1.133486  ],\n",
       "        [ 0.33843577,  0.10972001,  0.6814553 , ..., -0.38019413,\n",
       "         -1.4919765 ,  0.293065  ],\n",
       "        [ 0.4351084 ,  0.44374073,  0.7876026 , ..., -0.54122025,\n",
       "         -0.31613314,  0.4803167 ]], dtype=float32),\n",
       " array([[ 8.1258273e-01, -1.4553578e-01, -5.2823836e-01, ...,\n",
       "         -5.5799925e-01, -1.1621797e+00,  2.7167436e-01],\n",
       "        [ 1.4431961e+00,  4.8142123e-01, -8.5353512e-01, ...,\n",
       "         -1.3246323e-01, -5.6695729e-01,  3.9856920e-01],\n",
       "        [ 1.2136361e-01,  4.9559134e-01, -8.1867927e-01, ...,\n",
       "         -3.1611624e-01, -9.8271406e-01, -4.7156900e-01],\n",
       "        ...,\n",
       "        [ 7.3841190e-01,  2.6713213e-01, -6.7786264e-01, ...,\n",
       "          1.6303787e-01, -1.2209930e+00,  1.4365354e-01],\n",
       "        [ 7.5565451e-01,  1.1729981e-02,  8.7025249e-01, ...,\n",
       "         -3.5121053e-01, -5.8748102e-01, -7.0887886e-02],\n",
       "        [ 8.9640278e-01,  8.4413867e-04, -4.0254822e-01, ...,\n",
       "          2.4465663e-02, -9.9738163e-01,  8.2489550e-01]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.44478586, -0.28655863, -0.10807729, ..., -0.90000695,\n",
       "         -0.52228886,  0.54808503],\n",
       "        [ 1.0355208 ,  0.40055448,  0.22379814, ..., -0.2156638 ,\n",
       "         -0.91299975, -0.19149743],\n",
       "        ...,\n",
       "        [ 0.83456594, -0.18841773,  0.5793377 , ..., -0.17040886,\n",
       "         -0.6033796 ,  0.70765954],\n",
       "        [ 1.6373688 , -0.07057258, -0.3403034 , ..., -1.034435  ,\n",
       "         -0.8637714 ,  0.71643645],\n",
       "        [ 1.0697303 , -0.6546923 , -0.04688627, ..., -0.7477723 ,\n",
       "         -0.3965463 ,  0.30490908]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.44478586, -0.28655863, -0.10807729, ..., -0.90000695,\n",
       "         -0.52228886,  0.54808503],\n",
       "        [ 1.0355208 ,  0.40055448,  0.22379814, ..., -0.2156638 ,\n",
       "         -0.91299975, -0.19149743],\n",
       "        ...,\n",
       "        [ 0.83456594, -0.18841773,  0.5793377 , ..., -0.17040886,\n",
       "         -0.6033796 ,  0.70765954],\n",
       "        [ 1.6373688 , -0.07057258, -0.3403034 , ..., -1.034435  ,\n",
       "         -0.8637714 ,  0.71643645],\n",
       "        [ 1.0697303 , -0.6546923 , -0.04688627, ..., -0.7477723 ,\n",
       "         -0.3965463 ,  0.30490908]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 0.23024471, -1.198324  , -0.23742962, ..., -0.77526647,\n",
       "         -0.879528  ,  0.23312499],\n",
       "        [ 0.2944864 ,  0.09308026,  0.29740515, ..., -0.43847075,\n",
       "         -0.8585222 , -0.5245079 ],\n",
       "        ...,\n",
       "        [ 0.8356476 , -0.42286742,  0.15158059, ..., -0.30628353,\n",
       "         -0.62029594,  1.133486  ],\n",
       "        [ 0.33843577,  0.10972001,  0.6814553 , ..., -0.38019413,\n",
       "         -1.4919765 ,  0.293065  ],\n",
       "        [ 0.4351084 ,  0.44374073,  0.7876026 , ..., -0.54122025,\n",
       "         -0.31613314,  0.4803167 ]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 1.0094244 , -0.36550614,  0.55580133, ..., -0.14141968,\n",
       "         -0.7309282 , -0.8012084 ],\n",
       "        [ 1.2239685 , -0.49176064,  0.46507668, ...,  0.26418585,\n",
       "         -1.6338592 ,  0.55263144],\n",
       "        ...,\n",
       "        [ 1.4942163 , -0.10018992, -0.54207724, ..., -0.34626132,\n",
       "         -0.41872108,  0.88671464],\n",
       "        [ 0.59781367, -0.2843256 ,  0.29890925, ...,  0.46156463,\n",
       "         -1.0647542 ,  0.28932709],\n",
       "        [ 1.60522   , -0.35350272,  0.7359017 , ..., -0.43507338,\n",
       "         -0.3493849 , -0.2651203 ]], dtype=float32),\n",
       " array([[-0.68353266,  0.0651826 , -0.6007641 , ..., -0.0736664 ,\n",
       "         -0.14644288,  0.84311026],\n",
       "        [ 1.2239685 , -0.49176064,  0.46507668, ...,  0.26418585,\n",
       "         -1.6338592 ,  0.55263144],\n",
       "        [ 1.0094244 , -0.36550614,  0.55580133, ..., -0.14141968,\n",
       "         -0.7309282 , -0.8012084 ],\n",
       "        ...,\n",
       "        [ 1.4942163 , -0.10018992, -0.54207724, ..., -0.34626132,\n",
       "         -0.41872108,  0.88671464],\n",
       "        [ 0.59781367, -0.2843256 ,  0.29890925, ...,  0.46156463,\n",
       "         -1.0647542 ,  0.28932709],\n",
       "        [ 1.60522   , -0.35350272,  0.7359017 , ..., -0.43507338,\n",
       "         -0.3493849 , -0.2651203 ]], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-whitening\n",
    "https://kexue.fm/archives/8321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.042115Z",
     "start_time": "2021-05-16T12:21:31.739738Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel, bias = compute_kernel_bias(vecs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.061732Z",
     "start_time": "2021-05-16T12:21:32.043472Z"
    }
   },
   "outputs": [],
   "source": [
    "job_title_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "job_title_embeddings = job_title_encoder.get_embeddings(True)\n",
    "job_title_embeddings.to_pickle('data/embedding/job_title.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.199012Z",
     "start_time": "2021-05-16T12:21:32.062949Z"
    }
   },
   "outputs": [],
   "source": [
    "recruit_major_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "recruit_major_embeddings = recruit_major_encoder.get_embeddings(True)\n",
    "recruit_major_embeddings.to_pickle('data/embedding/recruit_major.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.328048Z",
     "start_time": "2021-05-16T12:21:32.201866Z"
    }
   },
   "outputs": [],
   "source": [
    "person_major_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "person_major_embeddings = person_major_encoder.get_embeddings(True)\n",
    "person_major_embeddings.to_pickle('data/embedding/person_major.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.467025Z",
     "start_time": "2021-05-16T12:21:32.330019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAJOR</th>\n",
       "      <th>person_MAJOR_ernie_emb_0</th>\n",
       "      <th>person_MAJOR_ernie_emb_1</th>\n",
       "      <th>person_MAJOR_ernie_emb_2</th>\n",
       "      <th>person_MAJOR_ernie_emb_3</th>\n",
       "      <th>person_MAJOR_ernie_emb_4</th>\n",
       "      <th>person_MAJOR_ernie_emb_5</th>\n",
       "      <th>person_MAJOR_ernie_emb_6</th>\n",
       "      <th>person_MAJOR_ernie_emb_7</th>\n",
       "      <th>person_MAJOR_ernie_emb_8</th>\n",
       "      <th>person_MAJOR_ernie_emb_9</th>\n",
       "      <th>person_MAJOR_ernie_emb_10</th>\n",
       "      <th>person_MAJOR_ernie_emb_11</th>\n",
       "      <th>person_MAJOR_ernie_emb_12</th>\n",
       "      <th>person_MAJOR_ernie_emb_13</th>\n",
       "      <th>person_MAJOR_ernie_emb_14</th>\n",
       "      <th>person_MAJOR_ernie_emb_15</th>\n",
       "      <th>person_MAJOR_ernie_emb_16</th>\n",
       "      <th>person_MAJOR_ernie_emb_17</th>\n",
       "      <th>person_MAJOR_ernie_emb_18</th>\n",
       "      <th>person_MAJOR_ernie_emb_19</th>\n",
       "      <th>person_MAJOR_ernie_emb_20</th>\n",
       "      <th>person_MAJOR_ernie_emb_21</th>\n",
       "      <th>person_MAJOR_ernie_emb_22</th>\n",
       "      <th>person_MAJOR_ernie_emb_23</th>\n",
       "      <th>person_MAJOR_ernie_emb_24</th>\n",
       "      <th>person_MAJOR_ernie_emb_25</th>\n",
       "      <th>person_MAJOR_ernie_emb_26</th>\n",
       "      <th>person_MAJOR_ernie_emb_27</th>\n",
       "      <th>person_MAJOR_ernie_emb_28</th>\n",
       "      <th>person_MAJOR_ernie_emb_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.013635</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>-0.057895</td>\n",
       "      <td>-0.023926</td>\n",
       "      <td>0.175738</td>\n",
       "      <td>0.164491</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>-0.110692</td>\n",
       "      <td>0.045256</td>\n",
       "      <td>0.148158</td>\n",
       "      <td>0.066279</td>\n",
       "      <td>-0.289184</td>\n",
       "      <td>-0.115000</td>\n",
       "      <td>-0.091854</td>\n",
       "      <td>0.140595</td>\n",
       "      <td>-0.233214</td>\n",
       "      <td>-0.377075</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.053626</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.321371</td>\n",
       "      <td>-0.043652</td>\n",
       "      <td>-0.307297</td>\n",
       "      <td>0.060946</td>\n",
       "      <td>-0.273020</td>\n",
       "      <td>0.047547</td>\n",
       "      <td>-0.126086</td>\n",
       "      <td>0.102405</td>\n",
       "      <td>-0.512496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>海洋生物学</td>\n",
       "      <td>0.210712</td>\n",
       "      <td>0.307610</td>\n",
       "      <td>-0.223881</td>\n",
       "      <td>-0.108800</td>\n",
       "      <td>0.065418</td>\n",
       "      <td>-0.162131</td>\n",
       "      <td>0.227934</td>\n",
       "      <td>-0.044880</td>\n",
       "      <td>-0.280816</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.145195</td>\n",
       "      <td>-0.073067</td>\n",
       "      <td>0.178802</td>\n",
       "      <td>0.053919</td>\n",
       "      <td>-0.089757</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.160066</td>\n",
       "      <td>0.160978</td>\n",
       "      <td>0.330783</td>\n",
       "      <td>-0.094730</td>\n",
       "      <td>0.493963</td>\n",
       "      <td>-0.118945</td>\n",
       "      <td>-0.048793</td>\n",
       "      <td>0.127638</td>\n",
       "      <td>-0.246648</td>\n",
       "      <td>0.155616</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>-0.082394</td>\n",
       "      <td>0.114455</td>\n",
       "      <td>-0.057680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>水力学及河流动力学</td>\n",
       "      <td>0.427427</td>\n",
       "      <td>0.193288</td>\n",
       "      <td>-0.259960</td>\n",
       "      <td>-0.169964</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>-0.019772</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>-0.187139</td>\n",
       "      <td>0.393858</td>\n",
       "      <td>0.095704</td>\n",
       "      <td>-0.066989</td>\n",
       "      <td>0.215284</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>0.132729</td>\n",
       "      <td>0.114047</td>\n",
       "      <td>-0.141971</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.099448</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>-0.131634</td>\n",
       "      <td>0.185809</td>\n",
       "      <td>-0.011383</td>\n",
       "      <td>-0.038974</td>\n",
       "      <td>-0.143077</td>\n",
       "      <td>0.277509</td>\n",
       "      <td>0.250184</td>\n",
       "      <td>-0.150335</td>\n",
       "      <td>-0.110450</td>\n",
       "      <td>0.189922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中国哲学</td>\n",
       "      <td>0.081370</td>\n",
       "      <td>0.195754</td>\n",
       "      <td>-0.173301</td>\n",
       "      <td>-0.339289</td>\n",
       "      <td>0.155924</td>\n",
       "      <td>0.241601</td>\n",
       "      <td>0.040983</td>\n",
       "      <td>0.115527</td>\n",
       "      <td>-0.484167</td>\n",
       "      <td>-0.147083</td>\n",
       "      <td>-0.125517</td>\n",
       "      <td>-0.006267</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.121235</td>\n",
       "      <td>-0.098301</td>\n",
       "      <td>0.218872</td>\n",
       "      <td>0.101693</td>\n",
       "      <td>-0.120968</td>\n",
       "      <td>0.266713</td>\n",
       "      <td>0.215141</td>\n",
       "      <td>-0.087461</td>\n",
       "      <td>-0.330836</td>\n",
       "      <td>0.029399</td>\n",
       "      <td>0.063886</td>\n",
       "      <td>0.050128</td>\n",
       "      <td>-0.020680</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>-0.117418</td>\n",
       "      <td>-0.019004</td>\n",
       "      <td>-0.024318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>电子自动化</td>\n",
       "      <td>-0.031067</td>\n",
       "      <td>0.260592</td>\n",
       "      <td>-0.053970</td>\n",
       "      <td>0.256489</td>\n",
       "      <td>-0.023287</td>\n",
       "      <td>-0.029740</td>\n",
       "      <td>-0.042982</td>\n",
       "      <td>0.142532</td>\n",
       "      <td>0.331002</td>\n",
       "      <td>0.047029</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.051545</td>\n",
       "      <td>-0.363207</td>\n",
       "      <td>-0.114460</td>\n",
       "      <td>0.261854</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>-0.231060</td>\n",
       "      <td>-0.069551</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>0.099890</td>\n",
       "      <td>0.256754</td>\n",
       "      <td>-0.228839</td>\n",
       "      <td>0.375351</td>\n",
       "      <td>-0.018511</td>\n",
       "      <td>-0.017693</td>\n",
       "      <td>0.240307</td>\n",
       "      <td>-0.188084</td>\n",
       "      <td>-0.128614</td>\n",
       "      <td>-0.037417</td>\n",
       "      <td>-0.134949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MAJOR  person_MAJOR_ernie_emb_0  person_MAJOR_ernie_emb_1  \\\n",
       "0                             0.013635                  0.020516   \n",
       "1      海洋生物学                  0.210712                  0.307610   \n",
       "2  水力学及河流动力学                  0.427427                  0.193288   \n",
       "3       中国哲学                  0.081370                  0.195754   \n",
       "4      电子自动化                 -0.031067                  0.260592   \n",
       "\n",
       "   person_MAJOR_ernie_emb_2  person_MAJOR_ernie_emb_3  \\\n",
       "0                 -0.057895                 -0.023926   \n",
       "1                 -0.223881                 -0.108800   \n",
       "2                 -0.259960                 -0.169964   \n",
       "3                 -0.173301                 -0.339289   \n",
       "4                 -0.053970                  0.256489   \n",
       "\n",
       "   person_MAJOR_ernie_emb_4  person_MAJOR_ernie_emb_5  \\\n",
       "0                  0.175738                  0.164491   \n",
       "1                  0.065418                 -0.162131   \n",
       "2                  0.056329                 -0.019772   \n",
       "3                  0.155924                  0.241601   \n",
       "4                 -0.023287                 -0.029740   \n",
       "\n",
       "   person_MAJOR_ernie_emb_6  person_MAJOR_ernie_emb_7  \\\n",
       "0                  0.016124                 -0.110692   \n",
       "1                  0.227934                 -0.044880   \n",
       "2                  0.016125                 -0.008364   \n",
       "3                  0.040983                  0.115527   \n",
       "4                 -0.042982                  0.142532   \n",
       "\n",
       "   person_MAJOR_ernie_emb_8  person_MAJOR_ernie_emb_9  \\\n",
       "0                  0.045256                  0.148158   \n",
       "1                 -0.280816                  0.105700   \n",
       "2                 -0.187139                  0.393858   \n",
       "3                 -0.484167                 -0.147083   \n",
       "4                  0.331002                  0.047029   \n",
       "\n",
       "   person_MAJOR_ernie_emb_10  person_MAJOR_ernie_emb_11  \\\n",
       "0                   0.066279                  -0.289184   \n",
       "1                   0.145195                  -0.073067   \n",
       "2                   0.095704                  -0.066989   \n",
       "3                  -0.125517                  -0.006267   \n",
       "4                  -0.033678                   0.051545   \n",
       "\n",
       "   person_MAJOR_ernie_emb_12  person_MAJOR_ernie_emb_13  \\\n",
       "0                  -0.115000                  -0.091854   \n",
       "1                   0.178802                   0.053919   \n",
       "2                   0.215284                   0.280064   \n",
       "3                   0.065417                   0.121235   \n",
       "4                  -0.363207                  -0.114460   \n",
       "\n",
       "   person_MAJOR_ernie_emb_14  person_MAJOR_ernie_emb_15  \\\n",
       "0                   0.140595                  -0.233214   \n",
       "1                  -0.089757                  -0.002913   \n",
       "2                   0.132729                   0.114047   \n",
       "3                  -0.098301                   0.218872   \n",
       "4                   0.261854                   0.139233   \n",
       "\n",
       "   person_MAJOR_ernie_emb_16  person_MAJOR_ernie_emb_17  \\\n",
       "0                  -0.377075                   0.047243   \n",
       "1                   0.160066                   0.160978   \n",
       "2                  -0.141971                   0.052865   \n",
       "3                   0.101693                  -0.120968   \n",
       "4                  -0.231060                  -0.069551   \n",
       "\n",
       "   person_MAJOR_ernie_emb_18  person_MAJOR_ernie_emb_19  \\\n",
       "0                   0.001328                  -0.053626   \n",
       "1                   0.330783                  -0.094730   \n",
       "2                   0.099448                   0.055683   \n",
       "3                   0.266713                   0.215141   \n",
       "4                  -0.181619                   0.099890   \n",
       "\n",
       "   person_MAJOR_ernie_emb_20  person_MAJOR_ernie_emb_21  \\\n",
       "0                   0.026738                   0.321371   \n",
       "1                   0.493963                  -0.118945   \n",
       "2                  -0.131634                   0.185809   \n",
       "3                  -0.087461                  -0.330836   \n",
       "4                   0.256754                  -0.228839   \n",
       "\n",
       "   person_MAJOR_ernie_emb_22  person_MAJOR_ernie_emb_23  \\\n",
       "0                  -0.043652                  -0.307297   \n",
       "1                  -0.048793                   0.127638   \n",
       "2                  -0.011383                  -0.038974   \n",
       "3                   0.029399                   0.063886   \n",
       "4                   0.375351                  -0.018511   \n",
       "\n",
       "   person_MAJOR_ernie_emb_24  person_MAJOR_ernie_emb_25  \\\n",
       "0                   0.060946                  -0.273020   \n",
       "1                  -0.246648                   0.155616   \n",
       "2                  -0.143077                   0.277509   \n",
       "3                   0.050128                  -0.020680   \n",
       "4                  -0.017693                   0.240307   \n",
       "\n",
       "   person_MAJOR_ernie_emb_26  person_MAJOR_ernie_emb_27  \\\n",
       "0                   0.047547                  -0.126086   \n",
       "1                   0.008727                  -0.082394   \n",
       "2                   0.250184                  -0.150335   \n",
       "3                   0.285808                  -0.117418   \n",
       "4                  -0.188084                  -0.128614   \n",
       "\n",
       "   person_MAJOR_ernie_emb_28  person_MAJOR_ernie_emb_29  \n",
       "0                   0.102405                  -0.512496  \n",
       "1                   0.114455                  -0.057680  \n",
       "2                  -0.110450                   0.189922  \n",
       "3                  -0.019004                  -0.024318  \n",
       "4                  -0.037417                  -0.134949  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_major_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_lasposition_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "person_lasposition_embeddings = person_lasposition_encoder.get_embeddings(True)\n",
    "person_lasposition_embeddings.to_pickle('data/embedding/person_lasposition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_cvposition_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "person_cvposition_embeddings = person_cvposition_encoder.get_embeddings(True)\n",
    "person_cvposition_embeddings.to_pickle('data/embedding/person_cvposition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_selfcomment_encoder.transform_and_normalize(kernel, bias, 30)\n",
    "# person_selfcomment_embeddings = person_selfcomment_encoder.get_embeddings(True)\n",
    "# person_selfcomment_embeddings.to_pickle('data/embedding/person_selfcomment.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算匹配度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.775360Z",
     "start_time": "2021-05-16T12:21:32.468152Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/trainset/recruit_folder.csv')\n",
    "df_test = pd.read_csv('data/testset/recruit_folder.csv')\n",
    "df_test['LABEL'] = np.nan\n",
    "df_feature = df_train.append(df_test, sort=False)\n",
    "df_recruit = pd.read_csv('data/trainset/recruit.csv')\n",
    "df_feature = df_feature.merge(df_recruit[['RECRUIT_ID', 'MAJOR']],\n",
    "                              how='left',\n",
    "                              on='RECRUIT_ID')\n",
    "df_feature.rename({'MAJOR': 'recruit_MAJOR'}, axis=1, inplace=True)\n",
    "df_person = pd.read_csv('data/trainset/person.csv')\n",
    "df_feature = df_feature.merge(df_person[['PERSON_ID', 'MAJOR']],\n",
    "                              how='left',\n",
    "                              on='PERSON_ID')\n",
    "df_feature.rename({'MAJOR': 'person_MAJOR'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.783148Z",
     "start_time": "2021-05-16T12:21:32.776579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECRUIT_ID</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>recruit_MAJOR</th>\n",
       "      <th>person_MAJOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>825081</td>\n",
       "      <td>6256839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>工业自动化</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772899</td>\n",
       "      <td>5413605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>旅游管理</td>\n",
       "      <td>文秘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>795668</td>\n",
       "      <td>5219796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>财政学（含税收学）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769754</td>\n",
       "      <td>5700693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>计算机应用技术</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>773645</td>\n",
       "      <td>6208645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>汽车工程</td>\n",
       "      <td>计算机应用技术</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RECRUIT_ID  PERSON_ID  LABEL recruit_MAJOR person_MAJOR\n",
       "0      825081    6256839    0.0         工业自动化          NaN\n",
       "1      772899    5413605    0.0          旅游管理           文秘\n",
       "2      795668    5219796    0.0           NaN    财政学（含税收学）\n",
       "3      769754    5700693    0.0           NaN      计算机应用技术\n",
       "4      773645    6208645    0.0          汽车工程      计算机应用技术"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:32.876316Z",
     "start_time": "2021-05-16T12:21:32.784215Z"
    }
   },
   "outputs": [],
   "source": [
    "def consine(vector1, vector2):\n",
    "    if type(vector1) != np.ndarray or type(vector2) != np.ndarray:\n",
    "        return -1\n",
    "    distance = np.dot(vector1, vector2) / \\\n",
    "        (np.linalg.norm(vector1)*(np.linalg.norm(vector2)))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:34.483997Z",
     "start_time": "2021-05-16T12:21:32.879917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_feature['recruit_person_MAJOR_score'] = df_feature[[\n",
    "    'recruit_MAJOR', 'person_MAJOR'\n",
    "]].apply(lambda x: consine(\n",
    "    recruit_major_encoder.get_embedding(x['recruit_MAJOR'], True),\n",
    "    person_major_encoder.get_embedding(x['person_MAJOR'], True)),\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T12:21:34.491790Z",
     "start_time": "2021-05-16T12:21:34.485365Z"
    }
   },
   "outputs": [],
   "source": [
    "df_feature[['RECRUIT_ID', 'PERSON_ID',\n",
    "            'recruit_person_MAJOR_score']].to_pickle('data/score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf 计算上一份工作职位和当前工作职位的匹配度\n",
    "df_train = pd.read_csv('data/trainset/recruit_folder.csv')\n",
    "df_test = pd.read_csv('data/testset/recruit_folder.csv')\n",
    "df_test['LABEL'] = np.nan\n",
    "df_feature = df_train.append(df_test, sort=False)\n",
    "df_recruit = pd.read_csv('data/trainset/recruit.csv')\n",
    "df_feature = df_feature.merge(df_recruit[['RECRUIT_ID', 'JOB_TITLE']],\n",
    "                              how='left',\n",
    "                              on='RECRUIT_ID')\n",
    "#df_feature.rename({'MAJOR': 'recruit_MAJOR'}, axis=1, inplace=True)\n",
    "df_person = pd.read_csv('data/trainset/person.csv')\n",
    "df_feature = df_feature.merge(df_person[['PERSON_ID', 'LAST_POSITION']],\n",
    "                              how='left',\n",
    "                              on='PERSON_ID')\n",
    "#df_feature.rename({'MAJOR': 'person_MAJOR'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECRUIT_ID</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>LAST_POSITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>825081</td>\n",
       "      <td>6256839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>非标结构工程师</td>\n",
       "      <td>机械设计</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772899</td>\n",
       "      <td>5413605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>航空机票售票员</td>\n",
       "      <td>人力资源管理</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>795668</td>\n",
       "      <td>5219796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>会计主管</td>\n",
       "      <td>会计</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769754</td>\n",
       "      <td>5700693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>普工/技术人员</td>\n",
       "      <td>*公关/营销/业务类</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>773645</td>\n",
       "      <td>6208645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>拖车司机</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RECRUIT_ID  PERSON_ID  LABEL JOB_TITLE LAST_POSITION\n",
       "0      825081    6256839    0.0   非标结构工程师          机械设计\n",
       "1      772899    5413605    0.0   航空机票售票员        人力资源管理\n",
       "2      795668    5219796    0.0      会计主管           会计 \n",
       "3      769754    5700693    0.0   普工/技术人员    *公关/营销/业务类\n",
       "4      773645    6208645    0.0      拖车司机           NaN"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature['recruit_person_POSITION_score'] = df_feature[[\n",
    "    'JOB_TITLE', 'LAST_POSITION'\n",
    "]].apply(lambda x: consine(\n",
    "    recruit_major_encoder.get_embedding(x['JOB_TITLE'], True),\n",
    "    person_lasposition_encoder.get_embedding(x['LAST_POSITION'], True)),\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature[['RECRUIT_ID', 'PERSON_ID',\n",
    "            'recruit_person_POSITION_score']].to_pickle('data/position_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf 计算上一份工作职位和投递的工作职位的匹配度\n",
    "df_train = pd.read_csv('data/trainset/recruit_folder.csv')\n",
    "df_test = pd.read_csv('data/testset/recruit_folder.csv')\n",
    "df_test['LABEL'] = np.nan\n",
    "df_feature = df_train.append(df_test, sort=False)\n",
    "df_recruit = pd.read_csv('data/trainset/person_cv.csv')\n",
    "df_feature = df_feature.merge(df_recruit[['PERSON_ID', 'POSITION']],\n",
    "                              how='left',\n",
    "                              on='PERSON_ID')\n",
    "#df_feature.rename({'MAJOR': 'recruit_MAJOR'}, axis=1, inplace=True)\n",
    "df_person = pd.read_csv('data/trainset/person.csv')\n",
    "df_feature = df_feature.merge(df_person[['PERSON_ID', 'LAST_POSITION']],\n",
    "                              how='left',\n",
    "                              on='PERSON_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECRUIT_ID</th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>POSITION</th>\n",
       "      <th>LAST_POSITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>825081</td>\n",
       "      <td>6256839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>*机械类</td>\n",
       "      <td>机械设计</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772899</td>\n",
       "      <td>5413605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>行政管理</td>\n",
       "      <td>人力资源管理</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>795668</td>\n",
       "      <td>5219796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>会计</td>\n",
       "      <td>会计</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769754</td>\n",
       "      <td>5700693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>技术支持</td>\n",
       "      <td>*公关/营销/业务类</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>773645</td>\n",
       "      <td>6208645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>汽车修理</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RECRUIT_ID  PERSON_ID  LABEL POSITION LAST_POSITION\n",
       "0      825081    6256839    0.0     *机械类          机械设计\n",
       "1      772899    5413605    0.0     行政管理        人力资源管理\n",
       "2      795668    5219796    0.0      会计            会计 \n",
       "3      769754    5700693    0.0     技术支持    *公关/营销/业务类\n",
       "4      773645    6208645    0.0     汽车修理           NaN"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature['personcv_person_POSITION_score'] = df_feature[[\n",
    "    'POSITION', 'LAST_POSITION'\n",
    "]].apply(lambda x: consine(\n",
    "    person_cvposition_encoder.get_embedding(x['POSITION'], True),\n",
    "    person_lasposition_encoder.get_embedding(x['LAST_POSITION'], True)),\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature[['PERSON_ID',\n",
    "            'personcv_person_POSITION_score']].to_pickle('data/peroncv_person_position_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RECRUIT_ID  PERSON_ID  LABEL POSITION LAST_POSITION  \\\n",
      "0      825081    6256839    0.0     *机械类          机械设计   \n",
      "1      772899    5413605    0.0     行政管理        人力资源管理   \n",
      "2      795668    5219796    0.0      会计            会计    \n",
      "3      769754    5700693    0.0     技术支持    *公关/营销/业务类   \n",
      "4      773645    6208645    0.0     汽车修理           NaN   \n",
      "\n",
      "   personcv_person_POSITION_score  \n",
      "0                       -1.000000  \n",
      "1                        0.656679  \n",
      "2                        1.000000  \n",
      "3                       -1.000000  \n",
      "4                       -1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_feature.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.5",
   "language": "python",
   "name": "torch1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
